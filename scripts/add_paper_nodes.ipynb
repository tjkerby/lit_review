{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "import sys\n",
    "\n",
    "sys.path.append('C:/Users/tjker/Desktop/Research/Projects/lit_review/lit_review')\n",
    "import semantic_scholar_api as ss_api\n",
    "import neo4j_utils as nu\n",
    "import kg_builder as kgb\n",
    "\n",
    "def load_config(config_file=\"C:/Users/tjker/Desktop/Research/Projects/lit_review/configs/build_kg.yaml\"):\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "kg = Neo4jGraph(\n",
    "    url=config['database']['uri'], \n",
    "    username=config['database']['username'], \n",
    "    password=config['database']['password'],\n",
    "    database=config['database']['database']\n",
    ")\n",
    "\n",
    "with open(config['file_paths']['paper_titles'], 'r') as file:\n",
    "    titles = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a542ce821944568233dcbd7a5a7895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ea1b738e2142779b87af96a8cd7bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceeded rate limit. Please try again later.\n"
     ]
    }
   ],
   "source": [
    "paper_data = ss_api.extract_paper_data(titles[:2])\n",
    "author_info = nu.create_paper_nodes(kg, paper_data, return_authors=True)\n",
    "if config['graph']['author']:\n",
    "    for paper in author_info:\n",
    "        kgb.build_author_data(kg, paper['authors'], paper['paperId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'p': {'citationCount': 28, 'level': 1, 'publicationTypes': ['JournalArticle', 'Conference'], 'abstract': \"Denoising Diffusion Models (DDMs) have emerged as a strong competitor to Generative Adversarial Networks (GANs). However, despite their widespread use in image synthesis and editing applications, their latent space is still not as well understood. Recently, a semantic latent space for DDMs, coined ‘ $h$ -space’, was shown to facilitate semantic image editing in a way reminiscent of GANs. The $h$ -space is comprised of the bottleneck activations in the DDM's denoiser across all timesteps of the diffusion process. In this paper, we explore the properties of $h$ -space and propose several novel methods for finding meaningful semantic directions within it. We start by studying unsupervised methods for revealing interpretable semantic directions in pretrained DDMs. Specifically, we show that interpretable directions emerge as the principal components in the latent space. Additionally, we provide a novel method for discovering image-specific semantic directions by spectral analysis of the Jacobian of the denoiser w.r.t. the latent code. Next, we extend the analysis by finding directions in a supervised fashion in unconditional DDMs. We demonstrate how such directions can be found by annotating generated samples with a domain-specific attribute classifier. We further show how to semantically disentangle the found directions by simple linear projection. Our approaches are applicable without requiring any architectural modifications, text-based guidance, CLIP-based optimization, or model fine-tuning.\", 'title': 'Discovering Interpretable Directions in the Semantic Latent Space of Diffusion Models', 'publicationDate': '2023-03-20', 'paperId': 'a64e9fe44051d93202853a43656def4b44f84883', 'url': 'https://www.semanticscholar.org/paper/a64e9fe44051d93202853a43656def4b44f84883'}}]\n",
      "[{'p': {'citationCount': 202, 'level': 1, 'publicationTypes': ['JournalArticle'], 'abstract': 'Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we introduce a principled design of the generative process for versatile editing and quality boost ing by quantifiable measures: editing strength of an interval and quality deficiency at a timestep. Our method is applicable to various architectures (DDPM++, iD- DPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN- bedroom, and METFACES). Project page: https://kwonminki.github.io/Asyrp/', 'title': 'Diffusion Models already have a Semantic Latent Space', 'publicationDate': '2022-10-20', 'paperId': 'a02313d56a6f71be9aafe43628e0f3a1d0cb858e', 'url': 'https://www.semanticscholar.org/paper/a02313d56a6f71be9aafe43628e0f3a1d0cb858e'}}]\n",
      "[{'p': {'citationCount': 27, 'level': 1, 'publicationTypes': ['JournalArticle', 'Conference'], 'abstract': 'Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for training and is thus dependent on their availability and correctness. In this paper, we eliminate the need for such annotation by instead exploiting the flexibility of self-supervision signals to design a framework for self-guided diffusion models. By leveraging a feature extraction function and a self-annotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels. When equipped with self-supervised box or mask proposals, our method further generates visually diverse yet semantically consistent images, without the need for any class, box, or segment label annotation. Self-guided diffusion is simple, flexible and expected to profit from deployment at scale.', 'title': 'Self-Guided Diffusion Models', 'publicationDate': '2022-10-12', 'paperId': 'b798c925a4c43ea09e76a1c748491ef70067c0c6', 'url': 'https://www.semanticscholar.org/paper/b798c925a4c43ea09e76a1c748491ef70067c0c6'}}]\n",
      "[{'p': {'citationCount': 1377, 'level': 1, 'publicationTypes': ['JournalArticle'], 'abstract': 'Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.', 'title': 'Prompt-to-Prompt Image Editing with Cross Attention Control', 'publicationDate': '2022-08-02', 'paperId': '04e541391e8dce14d099d00fb2c21dbbd8afe87f', 'url': 'https://www.semanticscholar.org/paper/04e541391e8dce14d099d00fb2c21dbbd8afe87f'}}]\n"
     ]
    }
   ],
   "source": [
    "if config['graph']['citation']:    \n",
    "    for paper in paper_data:\n",
    "        try:\n",
    "            citation_data = ss_api.exponential_backoff_retry(ss_api.get_paper_references, paper['paperId'], fields=config['graph']['citation_fields'])\n",
    "            for cited_paper in citation_data['data']:\n",
    "                cited_paper = cited_paper['citedPaper']\n",
    "                cited_paper_node = nu.search_papers_by_paperid(kg, cited_paper['paperId'])\n",
    "                if len(cited_paper_node) == 0:\n",
    "                    cited_paper['level'] = 2\n",
    "                    paper_properties = {key: value for key, value in cited_paper.items() if key != 'authors'}\n",
    "                    cited_paper_node = nu.create_paper_node(kg, paper_properties)\n",
    "                nu.create_cites_rel(kg, paper['paperId'], cited_paper['paperId'])     \n",
    "                if config['graph']['author']:\n",
    "                    kgb.build_author_data(kg, cited_paper['authors'], cited_paper['paperId'])\n",
    "        except ss_api.RateLimitExceededError:\n",
    "            print(\"Exceeded rate limit. Please try again later.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        # citation_data = kgb.build_citation_data(kg, config, paper['paperId'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
