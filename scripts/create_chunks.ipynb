{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append('C:/Users/tjker/Desktop/Research/Projects/lit_review/lit_review')\n",
    "import utils\n",
    "import rag_utils as rag\n",
    "import kg_builder as kgb\n",
    "\n",
    "sys.path.append('C:/Users/tjker/Desktop/Research/Projects/lit_review/configs')\n",
    "from create_chunks_config import config\n",
    "kg = utils.load_kg(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"\"\"\n",
    "    MATCH (c:Chunk) \n",
    "    DETACH DELETE c\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "kg.query(\"\"\" \n",
    "DROP INDEX paper_chunks IF EXISTS\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of this nice data organized we are going to load in each pdf and chunk it up and then create a Chunk node for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53f0114c9f54ccc985832a1bbfb0a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4118 nodes\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open(f\"{config['data']['data_path']}/{config['data']['paper_chunk_output_name']}\", 'r') as file:\n",
    "    updated_data = json.load(file)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = config['chunks']['chunk_size'],\n",
    "    chunk_overlap  = config['chunks']['chunk_overlap'],\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "kg.query(\"\"\"\n",
    "CREATE CONSTRAINT unique_chunk IF NOT EXISTS \n",
    "    FOR (c:Chunk) REQUIRE c.chunkId IS UNIQUE\n",
    "\"\"\")\n",
    "\n",
    "merge_chunk_node_query = \"\"\"\n",
    "MERGE(mergedChunk:Chunk {chunkId: $chunkParam.chunkId})\n",
    "    ON CREATE SET \n",
    "        mergedChunk.paperId = $chunkParam.paperId, \n",
    "        mergedChunk.source = $chunkParam.paperId,\n",
    "        mergedChunk.text = $chunkParam.text\n",
    "RETURN mergedChunk\n",
    "\"\"\"\n",
    "\n",
    "node_count = 0\n",
    "for paper in tqdm(updated_data):\n",
    "    chunks = kgb.paper_data_from_file(paper, text_splitter)\n",
    "    for chunk in chunks:\n",
    "        kg.query(merge_chunk_node_query, \n",
    "                params={\n",
    "                    'chunkParam': chunk\n",
    "                })\n",
    "        node_count += 1\n",
    "    # break\n",
    "print(f\"Created {node_count} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"\"\"\n",
    "  CREATE VECTOR INDEX `paper_chunks` IF NOT EXISTS\n",
    "  FOR (c:Chunk) ON (c.textEmbedding) \n",
    "  OPTIONS { indexConfig: {\n",
    "    `vector.dimensions`: $dimension,\n",
    "    `vector.similarity_function`: $similarity    \n",
    "  }}\"\"\", params={'dimension': config['embedding']['size'], 'similarity': config['embedding']['similarity']}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518272951ee14a0a8b73d831dc954d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(config['embedding']['model_id'])\n",
    "\n",
    "all_chunk_nodes = kg.query(\"\"\"\n",
    "    MATCH (c:Chunk) \n",
    "    RETURN elementId(c) AS chunk_id, c.text AS text\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "for record in tqdm(all_chunk_nodes):\n",
    "    chunk_id = record[\"chunk_id\"]\n",
    "    text = record[\"text\"]\n",
    "    \n",
    "    if text:\n",
    "        # embedding = rag.compute_embedding(text, tokenizer, model)\n",
    "        embedding = model.encode(text)\n",
    "        kg.query(\"\"\"\n",
    "            MATCH (c:Chunk) \n",
    "            WHERE elementId(c) = $chunk_id\n",
    "            SET c.textEmbedding = $embedding\n",
    "            RETURN elementId(c) AS chunk_id, c.textEmbedding AS embedding\n",
    "            \"\"\", params={\"chunk_id\":chunk_id, \"embedding\":embedding}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jVector\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import textwrap\n",
    "\n",
    "load_dotenv('C:/Users/tjker/Desktop/Research/Projects/lit_review/.env', override=True)\n",
    "\n",
    "llm = HuggingFaceEndpoint(model=config['model']['model_id'])\n",
    "\n",
    "chunk_vector = Neo4jVector.from_existing_index(\n",
    "    HuggingFaceEmbeddings(model_name=config['embedding']['model_id']),\n",
    "    graph=kg,\n",
    "    index_name='paper_chunks',\n",
    "    embedding_node_property='textEmbedding',\n",
    "    text_node_property='text',\n",
    ")\n",
    "\n",
    "retriever_20 = chunk_vector.as_retriever(search_kwargs={\"k\": 20})\n",
    "retriever_0 = chunk_vector.as_retriever()\n",
    "chunk_retriever = RetrievalQA.from_llm(llm=llm, retriever=retriever_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You can leverage the structure of a latent space to\n",
      "influence generation in various ways, such as manipulating\n",
      "the latent variables, using pre-trained visual encoders to\n",
      "modulate the capacity, injecting noise, or using\n",
      "crossattention modules to capture semantic relationships.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are some ways that you can leverage the structure of a latent space to influence generation?\"\n",
    "response = chunk_retriever.invoke({\"query\": question})\n",
    "\n",
    "print(textwrap.fill(response['result'], 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"How can we uncover the underlying structure of the latent space in GANs and Diffusion Models?\"\n",
    "# result = chunk_vector.similarity_search(question, k=3)\n",
    "# for doc in result:\n",
    "#     print(doc.metadata[\"chunkId\"], \"-\", doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever_0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': ' The structure of a latent space can be leveraged to influence the generation process through its evolution.\\n',\n",
       " 'sources': 'ca743e75ce090bbf686307e41bd8747661768fbe, d7074976c2609568902a6b6ca45f6c71d9cb66bf'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are some ways that the structure of a latent space can be leveraged to influence the generation process?\"\n",
    "response = chain.invoke({\"question\": question},\n",
    "        return_only_outputs=True,)\n",
    "\n",
    "# print(textwrap.fill(response['result'], 60))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vincent Tao Hu, David W Zhang, Yuki M Asano, Gertjan J\n",
      "Burghouts, and Cees G M Snoek.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "def prettychain(question: str) -> str:\n",
    "    \"\"\"Pretty print the chain's response to a question\"\"\"\n",
    "    response = chain.invoke({\"question\": question},\n",
    "        return_only_outputs=True,)\n",
    "    print(textwrap.fill(response['answer'], 60))\n",
    "    \n",
    "prettychain('who wrote Self-Guided Diffusion Models?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'who wrote Self-Guided Diffusion Models?',\n",
       " 'answer': ' Vincent Tao Hu, David W Zhang, Yuki M Asano, Gertjan J Burghouts, and Cees G M Snoek.\\n',\n",
       " 'sources': ''}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": 'who wrote Self-Guided Diffusion Models?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'who wrote DiGress: Discrete Denoising diffusion for graph generation?',\n",
       " 'answer': ' Denoising diffusion probabilistic models were written by Jonathan Ho, Ajay Jain, and Pieter Abbeel.\\n',\n",
       " 'sources': '33f3f31f871070f19b0c3e967a24e322bfc178f2, 33, 280, 22, 281, 23, and 282.'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": 'who wrote DiGress: Discrete Denoising diffusion for graph generation?'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
