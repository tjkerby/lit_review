{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f49df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/TomKerby/Research/lit_review/lit_review')\n",
    "import utils\n",
    "import rag_utils as rag\n",
    "from langchain_neo4j import Neo4jVector\n",
    "\n",
    "sys.path.append('/home/TomKerby/Research/lit_review/configs')\n",
    "# from rag_config import config\n",
    "from new_llm_config import config\n",
    "# from huggingface_rag import config\n",
    "\n",
    "kg = utils.load_kg(config)\n",
    "llm_adapter = rag.get_llm(config)\n",
    "emb_adapter = rag.get_embeddings(config)\n",
    "\n",
    "custom_query = \"\"\"\n",
    "MATCH (c:Chunk)\n",
    "WITH DISTINCT c, vector.similarity.cosine(c.textEmbedding, $embedding) AS score\n",
    "ORDER BY score DESC LIMIT $k\n",
    "RETURN c.text AS text, score, {source: c.source, chunkId: c.chunkId} AS metadata\n",
    "\"\"\"\n",
    "\n",
    "chunk_vector = Neo4jVector.from_existing_index(\n",
    "    emb_adapter.embeddings,\n",
    "    graph=kg, \n",
    "    index_name=config[\"rag\"][\"index_name\"],\n",
    "    embedding_node_property=config[\"rag\"][\"embedding_node_property\"],\n",
    "    text_node_property=config[\"rag\"][\"text_node_property\"],\n",
    "    retrieval_query=custom_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8360961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_retriever = chunk_vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6bd77fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "simple_rag_chain = (\n",
    "    {\"context\": neo4j_retriever, \"question\": RunnablePassthrough()}\n",
    "    | PromptTemplate.from_template(\"Answer: {context}\\nQuestion: {question}\")\n",
    "    | ChatOllama(model=\"llama3.3\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b896a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "A diffusion model is a type of probabilistic generative model that progressively destructs data by injecting noise and then learns to reverse this process for sample generation. In other words, it works by:\n",
      "\n",
      "1. Adding increasing amounts of random noise to the input data (called the \"diffusion\" process)\n",
      "2. Learning to remove the noise from the data to generate new samples\n",
      "\n",
      "The goal is to learn a model that can generate new data samples that are similar to the original data. Diffusion models have been shown to be effective in various applications, including computer vision, natural language processing, and more.\n",
      "\n",
      "There are several types of diffusion models, including:\n",
      "\n",
      "1. Denoising diffusion probabilistic models (DDPMs)\n",
      "2. Score-based generative models (SGMs)\n",
      "3. Stochastic differential equations (Score SDEs)\n",
      "\n",
      "These models work under the same principle of diffusion, but differ in their specific formulations and approaches to generating samples.\n",
      "\n",
      "In simple terms, a diffusion model is a type of machine learning model that learns to generate new data by adding noise to the original data and then removing it, rather than directly generating the data from scratch.\n"
     ]
    }
   ],
   "source": [
    "simple_rag_chain.invoke(\"What is a diffusion model?\").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain.tools import Tool, tool\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "# 1. Use the chat-style React template\n",
    "react_prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "\n",
    "# 2. Your two lookup tools…\n",
    "wiki = WikipediaRetriever()\n",
    "neokg = neo4j_retriever  # however you defined it\n",
    "\n",
    "tools = [\n",
    "    Tool(\"neo4j_kg_semantic_search\", lambda q: neokg.invoke(q), \"…\"),\n",
    "    Tool(\"wikipedia_search\", lambda q: wiki.invoke(q), \"…\"),\n",
    "]\n",
    "\n",
    "# 3. Add the “final answer” tool\n",
    "@tool(\"final_answer\", return_direct=True)\n",
    "def final_answer(text: str) -> str:\n",
    "    \"\"\"This tool helps the model exit infinite loops and give a final answer.\"\"\"\n",
    "    help_text = \"\\nThought: Do I need to use a tool? No\\nFinal Answer: \"\n",
    "    return help_text + text\n",
    "\n",
    "tools.append(final_answer)\n",
    "\n",
    "# 4. Create agent & executor\n",
    "agent = create_react_agent(\n",
    "    llm=ChatOllama(model=\"llama3.3\", temperature=0),\n",
    "    tools=tools,\n",
    "    prompt=react_prompt,\n",
    ")\n",
    "\n",
    "executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,  # keep this to auto-retry minor formatting slips\n",
    ")\n",
    "\n",
    "# 5. Run\n",
    "out = executor.invoke({\"input\": \"Please search the knowledge graph and use it to define what a diffusion model is?\", \"chat_history\": []})\n",
    "print(out[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e36e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_self_ask_with_search_agent, AgentExecutor\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain.tools import Tool, tool\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "# 2. Your two lookup tools…\n",
    "wiki = WikipediaRetriever()\n",
    "neokg = neo4j_retriever  # however you defined it\n",
    "\n",
    "tools = [\n",
    "    Tool(\"Intermediate Answer\", neokg.invoke, \"A tool for querying a knowledge graph using RAG.\")\n",
    "]\n",
    "\n",
    "\n",
    "# 4. Create agent & executor\n",
    "agent = create_self_ask_with_search_agent(\n",
    "    llm=ChatOllama(model=\"llama3.3\", temperature=0),\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\"input\": \"What is a diffusion model and how does it differ from a GAN?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcefd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.agents import create_self_ask_with_search_agent, AgentExecutor\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain.tools import Tool\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "# 1. Tools (must be exactly one, named \"Intermediate Answer\")\n",
    "wiki = WikipediaRetriever()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=wiki.invoke,\n",
    "        description=\"Useful for answering subquestions via search\"\n",
    "    )\n",
    "]\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"agent_scratchpad\"],\n",
    "    template=\"\"\"\n",
    "{agent_scratchpad}\n",
    "Question: {input}\n",
    "\n",
    "Think step-by-step:\n",
    "- If you need extra information, output exactly:\n",
    "  Follow up: <subquestion>\n",
    "- Otherwise, output exactly:\n",
    "  So the final answer is: <your answer>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Agent and executor with error handling\n",
    "agent = create_self_ask_with_search_agent(\n",
    "    llm=ChatOllama(model=\"llama3.3\", temperature=0),\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors='generate',\n",
    "    early_stopping_method='generate',\n",
    ")\n",
    "\n",
    "# 4. Run\n",
    "result = executor.invoke({\"input\": \"Can you describe in detail what is a diffusion model and how does it differ from a GAN?\"})\n",
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c34e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = executor.invoke({\"input\": \"Who wrote the paper titled 'DiffMat: Latent diffusion models for image-guided material generation'\"})\n",
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44743414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia_search` with `Diffusion model`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[Document(metadata={'title': 'Diffusion model', 'summary': 'In machine learning, diffusion models, also known as diffusion probabilistic models or score-based generative models, are a class of latent variable generative models. A diffusion model consists of three major components: the forward process, the reverse process, and the sampling procedure. The goal of diffusion models is to learn a diffusion process for a given dataset, such that the process can generate new elements that are distributed similarly as the original dataset. A diffusion model models data as generated by a diffusion process, whereby a new datum performs a random walk with drift through the space of all possible data. A trained diffusion model can be sampled in many ways, with different efficiency and quality.\\nThere are various equivalent formalisms, including Markov chains, denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. They are typically trained using variational inference. The model responsible for denoising is typically called its \"backbone\". The backbone may be of any kind, but they are typically U-nets or transformers.\\nAs of 2024, diffusion models are mainly used for computer vision tasks, including image denoising, inpainting, super-resolution, image generation, and video generation. These typically involve training a neural network to sequentially denoise images blurred with Gaussian noise. The model is trained to reverse the process of adding noise to an image. After training to convergence, it can be used for image generation by starting with an image composed of random noise, and applying the network iteratively to denoise the image.\\nDiffusion-based image generators have seen widespread commercial interest, such as Stable Diffusion and DALL-E. These models typically combine diffusion models with other models, such as text-encoders and cross-attention modules to allow text-conditioned generation.\\nOther than computer vision, diffusion models have also found applications in natural language processing such as text generation and summarization, sound generation, and reinforcement learning.', 'source': 'https://en.wikipedia.org/wiki/Diffusion_model'}, page_content='In machine learning, diffusion models, also known as diffusion probabilistic models or score-based generative models, are a class of latent variable generative models. A diffusion model consists of three major components: the forward process, the reverse process, and the sampling procedure. The goal of diffusion models is to learn a diffusion process for a given dataset, such that the process can generate new elements that are distributed similarly as the original dataset. A diffusion model models data as generated by a diffusion process, whereby a new datum performs a random walk with drift through the space of all possible data. A trained diffusion model can be sampled in many ways, with different efficiency and quality.\\nThere are various equivalent formalisms, including Markov chains, denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. They are typically trained using variational inference. The model responsible for denoising is typically called its \"backbone\". The backbone may be of any kind, but they are typically U-nets or transformers.\\nAs of 2024, diffusion models are mainly used for computer vision tasks, including image denoising, inpainting, super-resolution, image generation, and video generation. These typically involve training a neural network to sequentially denoise images blurred with Gaussian noise. The model is trained to reverse the process of adding noise to an image. After training to convergence, it can be used for image generation by starting with an image composed of random noise, and applying the network iteratively to denoise the image.\\nDiffusion-based image generators have seen widespread commercial interest, such as Stable Diffusion and DALL-E. These models typically combine diffusion models with other models, such as text-encoders and cross-attention modules to allow text-conditioned generation.\\nOther than computer vision, diffusion models have also found applications in natural language processing such as text generation and summarization, sound generation, and reinforcement learning.\\n\\n\\n== Denoising diffusion model ==\\n\\n\\n=== Non-equilibrium thermodynamics ===\\nDiffusion models were introduced in 2015 as a method to train a model that can sample from a highly complex probability distribution. They used techniques from non-equilibrium thermodynamics, especially diffusion.\\nConsider, for example, how one might model the distribution of all naturally-occurring photos. Each image is a point in the space of all images, and the distribution of naturally-occurring photos is a \"cloud\" in space, which, by repeatedly adding noise to the images, diffuses out to the rest of the image space, until the cloud becomes all but indistinguishable from a Gaussian distribution \\n  \\n    \\n      \\n        \\n          \\n            N\\n          \\n        \\n        (\\n        0\\n        ,\\n        I\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {N}}(0,I)}\\n  \\n. A model that can approximately undo the diffusion can then be used to sample from the original distribution. This is studied in \"non-equilibrium\" thermodynamics, as the starting distribution is not in equilibrium, unlike the final distribution.\\nThe equilibrium distribution is the Gaussian distribution \\n  \\n    \\n      \\n        \\n          \\n            N\\n          \\n        \\n        (\\n        0\\n        ,\\n        I\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {N}}(0,I)}\\n  \\n, with pdf \\n  \\n    \\n      \\n        ρ\\n        (\\n        x\\n        )\\n        ∝\\n        \\n          e\\n          \\n            −\\n            \\n              \\n                1\\n                2\\n              \\n            \\n            ‖\\n            x\\n            \\n              ‖\\n              \\n                2\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\rho (x)\\\\propto e^{-{\\\\frac {1}{2}}\\\\|x\\\\|^{2}}}\\n  \\n. This is just the Maxwell–Boltzmann distribution of particles in a potential well \\n  \\n    \\n      \\n        V\\n        ('), Document(metadata={'title': 'Latent diffusion model', 'summary': 'The Latent Diffusion Model (LDM) is a diffusion model architecture developed by the CompVis (Computer Vision & Learning) group at LMU Munich.\\nIntroduced in 2015, diffusion models (DMs) are trained with the objective of removing successive applications of noise (commonly Gaussian) on training images. The LDM is an improvement on standard DM by performing diffusion modeling in a latent space, and by allowing self-attention and cross-attention conditioning.\\nLDMs are widely used in practical diffusion models. For instance, Stable Diffusion versions 1.1 to 2.1 were based on the LDM architecture.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Latent_diffusion_model'}, page_content='The Latent Diffusion Model (LDM) is a diffusion model architecture developed by the CompVis (Computer Vision & Learning) group at LMU Munich.\\nIntroduced in 2015, diffusion models (DMs) are trained with the objective of removing successive applications of noise (commonly Gaussian) on training images. The LDM is an improvement on standard DM by performing diffusion modeling in a latent space, and by allowing self-attention and cross-attention conditioning.\\nLDMs are widely used in practical diffusion models. For instance, Stable Diffusion versions 1.1 to 2.1 were based on the LDM architecture.\\n\\n\\n== Version history ==\\nDiffusion models were introduced in 2015 as a method to learn a model that can sample from a highly complex probability distribution. They used techniques from non-equilibrium thermodynamics, especially diffusion. It was accompanied by a software implementation in Theano.\\nA 2019 paper proposed the noise conditional score network (NCSN) or score-matching with Langevin dynamics (SMLD). The paper was accompanied by a software package written in PyTorch release on GitHub.\\nA 2020 paper proposed the Denoising Diffusion Probabilistic Model (DDPM), which improves upon the previous method  by variational inference. The paper was accompanied by a software package written in TensorFlow release on GitHub. It was reimplemented in PyTorch by lucidrains.\\nOn December 20, 2021, the LDM paper was published on arXiv, and both Stable Diffusion and LDM repositories were published on GitHub. However, they remained roughly the same. Substantial information concerning Stable Diffusion v1 was only added to GitHub on August 10, 2022.\\nAll of Stable Diffusion (SD) versions 1.1 to XL were particular instantiations of the LDM architecture. \\nSD 1.1 to 1.4 were released by CompVis in August 2022. There is no \"version 1.0\". SD 1.1 was a LDM trained on the laion2B-en dataset. SD 1.1 was finetuned to 1.2 on more aesthetic images. SD 1.2 was finetuned to 1.3, 1.4 and 1.5, with 10% of text-conditioning dropped, to improve classifier-free guidance. SD 1.5 was released by RunwayML in October 2022.\\n\\n\\n== Architecture ==\\nWhile the LDM can work for generating arbitrary data conditional on arbitrary data, for concreteness, we describe its operation in conditional text-to-image generation.\\nLDM consists of a variational autoencoder (VAE), a modified U-Net, and a text encoder.\\nThe VAE encoder compresses the image from pixel space to a smaller dimensional latent space, capturing a more fundamental semantic meaning of the image. Gaussian noise is iteratively applied to the compressed latent representation during forward diffusion. The U-Net block, composed of a ResNet backbone, denoises the output from forward diffusion backwards to obtain a latent representation. Finally, the VAE decoder generates the final image by converting the representation back into pixel space.\\nThe denoising step can be conditioned on a string of text, an image, or another modality. The encoded conditioning data is exposed to denoising U-Nets via a cross-attention mechanism. For conditioning on text, the fixed, a pretrained CLIP ViT-L/14 text encoder is used to transform text prompts to an embedding space.\\n\\n\\n=== Variational Autoencoder ===\\nTo compress the image data, a variational autoencoder (VAE) is first trained on a dataset of images. The encoder part of the VAE takes an image as input and outputs a lower-dimensional latent representation of the image. This latent representation is then used as input to the U-Net. Once the model is trained, the encoder is used to encode images into latent representations, and the decoder is used to decode latent representations back into images.\\nLet the encoder and the decoder of the VAE be \\n  \\n    \\n      \\n        E\\n        ,\\n        D\\n      \\n    \\n    {\\\\displaystyle E,D}\\n  \\n.\\nTo encode an RGB image, its three channels are divided by the maximum value, resulting in a tensor \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  \\n of shape \\n  \\n    '), Document(metadata={'title': 'Stable Diffusion', 'summary': 'Stable Diffusion is a deep learning, text-to-image model released in 2022 based on diffusion techniques. The generative artificial intelligence technology is the premier product of Stability AI and is considered to be a part of the ongoing artificial intelligence boom.\\nIt is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt. Its development involved  researchers from the CompVis Group at Ludwig Maximilian University of Munich and Runway with a computational donation from Stability and training data from non-profit organizations.\\nStable Diffusion is a latent diffusion model, a kind of deep generative artificial neural network. Its code and model weights have been released publicly, and an optimized version can run on most consumer hardware equipped with a modest GPU with as little as 2.4 GB VRAM. This marked a departure from previous proprietary text-to-image models such as DALL-E and Midjourney which were accessible only via cloud services.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Stable_Diffusion'}, page_content='Stable Diffusion is a deep learning, text-to-image model released in 2022 based on diffusion techniques. The generative artificial intelligence technology is the premier product of Stability AI and is considered to be a part of the ongoing artificial intelligence boom.\\nIt is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt. Its development involved  researchers from the CompVis Group at Ludwig Maximilian University of Munich and Runway with a computational donation from Stability and training data from non-profit organizations.\\nStable Diffusion is a latent diffusion model, a kind of deep generative artificial neural network. Its code and model weights have been released publicly, and an optimized version can run on most consumer hardware equipped with a modest GPU with as little as 2.4 GB VRAM. This marked a departure from previous proprietary text-to-image models such as DALL-E and Midjourney which were accessible only via cloud services.\\n\\n\\n== Development ==\\nStable Diffusion originated from a project called Latent Diffusion, developed in Germany by researchers at Ludwig Maximilian University in Munich and Heidelberg University. Four of the original 5 authors (Robin Rombach, Andreas Blattmann, Patrick Esser and Dominik Lorenz) later joined Stability AI and released subsequent versions of Stable Diffusion.\\nThe technical license for the model was released by the CompVis group at Ludwig Maximilian University of Munich. Development was led by Patrick Esser of Runway and Robin Rombach of CompVis, who were among the researchers who had earlier invented the latent diffusion model architecture used by Stable Diffusion. Stability AI also credited EleutherAI and LAION (a German nonprofit which assembled the dataset on which Stable Diffusion was trained) as supporters of the project.\\n\\n\\n== Technology ==\\n\\n\\n=== Architecture ===\\n\\nDiffusion models, introduced in 2015, are trained with the objective of removing successive applications of Gaussian noise on training images, which can be thought of as a sequence of denoising autoencoders. The name diffusion is from the thermodynamic diffusion, since they were first developed with inspiration from thermodynamics.\\nModels in Stable Diffusion series before SD 3 all used a variant of diffusion models, called latent diffusion model (LDM), developed in 2021 by the CompVis (Computer Vision & Learning) group at LMU Munich. \\nStable Diffusion consists of 3 parts: the variational autoencoder (VAE), U-Net, and an optional text encoder. The VAE encoder compresses the image from pixel space to a smaller dimensional latent space, capturing a more fundamental semantic meaning of the image. Gaussian noise is iteratively applied to the compressed latent representation during forward diffusion. The U-Net block, composed of a ResNet backbone, denoises the output from forward diffusion backwards to obtain a latent representation. Finally, the VAE decoder generates the final image by converting the representation back into pixel space.\\nThe denoising step can be flexibly conditioned on a string of text, an image, or another modality. The encoded conditioning data is exposed to denoising U-Nets via a cross-attention mechanism. For conditioning on text, the fixed, pretrained CLIP ViT-L/14 text encoder is used to transform text prompts to an embedding space. Researchers point to increased computational efficiency for training and generation as an advantage of LDMs.\\nWith 860 million parameters in the U-Net and 123 million in the text encoder, Stable Diffusion is considered relatively lightweight by 2022 standards, and unlike other diffusion models, it can run on consumer GPUs, and even CPU-only if using the OpenVINO version of Stable Diffusion.\\n\\n\\n==== SD XL ====\\nThe XL version uses the same LDM architecture as previous versions, except larger: larger UNe')]\u001b[0m\u001b[32;1m\u001b[1;3mUn modelo de difusión es un tipo de modelo generativo de aprendizaje automático que se utiliza para generar imágenes o otros tipos de datos a partir de una descripción textual. Estos modelos están basados en la técnica de difusión, que implica agregar ruido a las imágenes y luego eliminarlo para generar nuevas imágenes. Los modelos de difusión han sido utilizados con éxito en tareas como la generación de imágenes, la inpainting, la superresolución y la traducción de imágenes. Algunos ejemplos de modelos de difusión son el modelo de difusión estable (Stable Diffusion) y el modelo de difusión latente (Latent Diffusion Model). Estos modelos han sido desarrollados por investigadores en universidades y empresas, y han sido publicados en conferencias y revistas científicas.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is a diffusion model?',\n",
       " 'chat_history': [],\n",
       " 'output': 'Un modelo de difusión es un tipo de modelo generativo de aprendizaje automático que se utiliza para generar imágenes o otros tipos de datos a partir de una descripción textual. Estos modelos están basados en la técnica de difusión, que implica agregar ruido a las imágenes y luego eliminarlo para generar nuevas imágenes. Los modelos de difusión han sido utilizados con éxito en tareas como la generación de imágenes, la inpainting, la superresolución y la traducción de imágenes. Algunos ejemplos de modelos de difusión son el modelo de difusión estable (Stable Diffusion) y el modelo de difusión latente (Latent Diffusion Model). Estos modelos han sido desarrollados por investigadores en universidades y empresas, y han sido publicados en conferencias y revistas científicas.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain.tools import Tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are a question-answering assistant.  \n",
    "You have access to two tools:  \n",
    "  • wikipedia_search(query): retrieves facts from Wikipedia  \n",
    "  • neo4j_kg_semantic_search(query): retrieves facts from our knowledge graph  \n",
    "\n",
    "**Workflow:**  \n",
    "1. Think whether you need to call a tool.  \n",
    "2. If yes, emit a tool call.  \n",
    "3. After retrieving info, answer the user's question directly.  \n",
    "\"\"\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.3\",\n",
    "    temperature=0,\n",
    "    num_ctx=131072,\n",
    ")\n",
    "\n",
    "wiki = WikipediaRetriever()\n",
    "\n",
    "tools = [\n",
    "    Tool(\"neo4j_kg_semantic_search\", lambda q: neo4j_retriever.invoke(q), \"A tool for searching a knowledge graph containing academic papers relating to the query.\"),\n",
    "    Tool(\"wikipedia_search\", lambda q: wiki.invoke(q), \"A tool for searching wikipedia.\"),\n",
    "]\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is a diffusion model?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cca8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is a diffusion model?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
