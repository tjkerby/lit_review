{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/tjker/Desktop/Research/Projects/lit_review/lit_review')\n",
    "import utils\n",
    "\n",
    "sys.path.append('C:/Users/tjker/Desktop/Research/Projects/lit_review/configs')\n",
    "from create_chunks_config import config\n",
    "\n",
    "kg = utils.load_kg(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8e4eb58e8a4f84a2776799b84ec798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_neo4j import Neo4jVector\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "# from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import textwrap\n",
    "\n",
    "load_dotenv('C:/Users/tjker/Desktop/Research/Projects/lit_review/.env', override=True)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoConfig\n",
    "\n",
    "\n",
    "# model_config = AutoConfig.from_pretrained(config['model']['model_id'], trust_remote_code=True)\n",
    "# model_config.max_seq_len = 4096\n",
    "# config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  config['model']['model_id'],\n",
    "  # config=model_config,\n",
    "  trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model']['model_id'], trust_remote_code=True)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "# tokenizer.model_max_length = 4096\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    'text-generation', \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "chunk_vector = Neo4jVector.from_existing_index(\n",
    "    HuggingFaceEmbeddings(model_name=config['embedding']['model_id'], model_kwargs={'trust_remote_code':True}),\n",
    "    graph=kg,\n",
    "    index_name='paper_chunks',\n",
    "    embedding_node_property='textEmbedding',\n",
    "    text_node_property='text',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Powerful Querying capability\n",
    "A query like the following:\n",
    "```\n",
    "MATCH (p:Paper)-[:HAS_CHUNK]->(c:Chunk)\n",
    "WHERE p.publication_date >= $min_date AND p.author = $desired_author\n",
    "WITH c, p, vector.similarity.cosine(c.embedding, $embedding) AS score\n",
    "ORDER BY score DESC LIMIT $k\n",
    "RETURN c, score, {title: p.title, url: p.url} AS metadata\n",
    "```\n",
    "\n",
    "Allows me to first filter the vector database that I look at and then perform RAG. This is far more targeted than looking at all chunks in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA with sources retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_response(response):\n",
    "    print(\"Question:\")\n",
    "    print(response.get(\"question\", \"\"))\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(response.get(\"answer\", \"\"))\n",
    "    print(\"\\nSources:\")\n",
    "    # If your sources key contains a string or list, print it nicely\n",
    "    sources = response.get(\"sources\", [])\n",
    "    if isinstance(sources, list):\n",
    "        for i, src in enumerate(sources, start=1):\n",
    "            print(f\"  [{i}] {src}\")\n",
    "    else:\n",
    "        print(sources)\n",
    "    print(\"\\nDetailed Source Documents:\")\n",
    "    docs = response.get(\"source_documents\", [])\n",
    "    for i, doc in enumerate(docs, start=1):\n",
    "        # Customize as needed; here we assume each doc is a Document object with metadata\n",
    "        title = doc.metadata.get(\"source\", \"Unknown Source\")\n",
    "        text_snippet = doc.page_content.replace(\"\\n\", \" \") + \"...\"\n",
    "        print(f\"Source {i}: {title}\")\n",
    "        print(f\"Snippet: {text_snippet}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\n",
    "\n",
    "custom_query = \"\"\"\n",
    "MATCH (c:Chunk)\n",
    "WITH DISTINCT c, vector.similarity.cosine(c.textEmbedding, $embedding) AS score\n",
    "ORDER BY score DESC LIMIT $k\n",
    "RETURN c.text AS text, score, {source: c.source, chunkId: c.chunkId} AS metadata\n",
    "\"\"\"\n",
    "\n",
    "filtered_chunk_vector = Neo4jVector.from_existing_index(\n",
    "    HuggingFaceEmbeddings(model_name=config['embedding']['model_id'], model_kwargs={'trust_remote_code':True}),\n",
    "    graph=kg,\n",
    "    index_name='paper_chunks',\n",
    "    embedding_node_property='textEmbedding',\n",
    "    text_node_property='text',\n",
    "    retrieval_query=custom_query,\n",
    ")\n",
    "\n",
    "retriever_k = filtered_chunk_vector.as_retriever(search_kwargs={\n",
    "        \"k\": 3\n",
    "    })\n",
    "\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever_k,\n",
    "    return_source_documents=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2847 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What are some ways that you can leverage the structure of a latent space to influence generation?\n",
      "\n",
      "Answer:\n",
      "Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "ALWAYS return a \"SOURCES\" part in your answer.\n",
      "\n",
      "\n",
      "\n",
      "Sources:\n",
      "Which state/country's law governs the interpretation of the contract?\n",
      "\n",
      "Detailed Source Documents:\n",
      "Source 1: ca743e75ce090bbf686307e41bd8747661768fbe\n",
      "Snippet: search_document: 2022; Hu et al. 2023c) has played a significant role in driving these advancements, prompting further investigations into the understanding of the learned latent space and its potential use for image editing tasks. While current works perform latent space editing on the original latent diffusion model setting and architecture (Kwon, Jeong, and Uh 2023; Haas et al. 2023), not much is known about the structure of the latent space in the most recent advances in the field, specifically  Copyright © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved....\n",
      "\n",
      "Source 2: e27b881192cd14a04018c372352b12e531870fc1\n",
      "Snippet: search_document: 2020a; Wu et al. 2021], such as changing facial orientations, expressions, or age, by traversing the learned manifold. While impressive, these edits are performed strictly in the generator’s latent space and cannot be applied to real images, i.e., not created by the trained generator. Hence, editing a real image starts with finding its latent representation. This process, called GAN _inversion, has recently drawn considerable attention [Abdal et al._ 2019; Alaluf et al. 2021; Menon et al....\n",
      "\n",
      "Source 3: d7074976c2609568902a6b6ca45f6c71d9cb66bf\n",
      "Snippet: search_document: Second, we investigate how the latent structures differ across different timesteps and samples as follows. The frequency domain of the local latent basis shifts from low-frequency to high-frequency along the generative process. We explicitly confirm it using power spectral density analysis. The difference between local tangent spaces of different samples becomes larger along the generative process. The local tangent spaces at various diffusion timesteps are similar to each other if the model is trained on aligned datasets such as CelebA-HQ or Flowers. However, this homogeneity does not occur on complex datasets such as ImageNet. Finally, we examine how the prompts affect the latent structure of text-to-image DMs as follows. Similar prompts yield similar latent structures. Specifically, we find a positive correlation between the similarity of prompts and the similarity of local tangent spaces. The influence of text on the local tangent space becomes weaker along the generative process. Our work examines the geometry of X and H using Riemannian geometry. We discover the latent structure of X and how it evolves during the generative process and is influenced by prompts. This geometric exploration deepens our understanding of DMs. ## 2 Related works  **Diffusion Models.** Recent advances in DMs make great progress in the field of image synthesis and show state-of-the-art performance [50, 22, 51]. An important subject in the diffusion model is the introduction of gradient guidance, including classifier-free guidance, to control the generative process [17, 47, 4, 30, 36, 46]. The work by Song et al. [52] has facilitated the unification of DMs with score-based models using SDEs, enhancing our understanding of DMs as a reverse diffusion process. However, the latent space is still largely unexplored, and our understanding is limited. **The study of latent space in GANs.** The study of latent spaces has gained significant attention in recent years. In the field of Generative Adversarial Networks (GANs), researchers have proposed various methods to manipulate the latent space to achieve the desired effect in the generated images   -----  [44, 41, 1, 20, 49, 59, 38]. More recently, several studies [60, 10] have examined the geometrical properties of latent space in GANs and utilized these findings for image manipulations. These studies bring the advantage of better understanding the characteristics of the latent space and facilitating the analysis and utilization of GANs. In contrast, the latent space of DMs remains poorly understood, making it difficult to fully utilize their capabilities. **Image manipulation in DMs.** Early works include Choi et al. [11] and Meng et al. [33] have attempted to manipulate the resulting images of DMs by replacing latent variables, allowing the generation of desired random images. However, due to the lack of semantics in the latent variables of DMs, current approaches have critical problems with semantic image editing. Alternative approaches have explored the potential of using the feature space within the U-Net for semantic image manipulation. For example, Kwon et al. [26] have shown that the bottleneck of the U-Net, H, can be used as a semantic latent space. Specifically, they used CLIP [43] to identify directions within H that facilitate genuine image editing....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What are some ways that you can leverage the structure of a latent space to influence generation?\"\n",
    "response = qa_chain.invoke({\"question\": question})\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textwrap.fill(response['answer'], 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are some ways that you can leverage the structure of a latent space to influence generation?\"\n",
    "response = chunk_retriever.invoke({\"query\": question})\n",
    "\n",
    "print(textwrap.fill(response['result'], 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"How can we uncover the underlying structure of the latent space in GANs and Diffusion Models?\"\n",
    "# result = chunk_vector.similarity_search(question, k=3)\n",
    "# for doc in result:\n",
    "#     print(doc.metadata[\"chunkId\"], \"-\", doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever_0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are some ways that the structure of a latent space can be leveraged to influence the generation process?\"\n",
    "response = chain.invoke({\"question\": question},\n",
    "        return_only_outputs=True,)\n",
    "\n",
    "# print(textwrap.fill(response['result'], 60))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def prettychain(question: str) -> str:\n",
    "    \"\"\"Pretty print the chain's response to a question\"\"\"\n",
    "    response = chain.invoke({\"question\": question},\n",
    "        return_only_outputs=True,)\n",
    "    print(textwrap.fill(response['answer'], 60))\n",
    "    \n",
    "prettychain('who wrote Self-Guided Diffusion Models?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": 'who wrote Self-Guided Diffusion Models?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": 'who wrote DiGress: Discrete Denoising diffusion for graph generation?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old graph_rag.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():     \n",
    "        outputs = model(**inputs) \n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze(0).tolist()\n",
    "\n",
    "question = \"Transformer architecture\"\n",
    "# question_embedding = model.encode(question)\n",
    "question_embedding = compute_embedding(question)\n",
    "\n",
    "kg.query(\"\"\"\n",
    "    CALL db.index.vector.queryNodes(\n",
    "        'abstract_embeddings', \n",
    "        $top_k, \n",
    "        $question_embedding\n",
    "        ) YIELD node AS paper, score\n",
    "    RETURN paper.title, paper.abstract, score\n",
    "    \"\"\", \n",
    "    params={\"top_k\":5,\n",
    "            \"question_embedding\": question_embedding\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Latent Space Editing in Transformer-Based Flow Matching'\n",
    "kg.query(\"\"\"\n",
    "    CALL db.index.fulltext.queryNodes('paperTitleIndex', $title)     \n",
    "    YIELD node, score\n",
    "    RETURN node.paperId, score\n",
    "    LIMIT 1\n",
    "    \"\"\", params={'title': title}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
